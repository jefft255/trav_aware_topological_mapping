import torch
import numpy as np
from torch.utils.data import Dataset
from typing import List, Any
from pathlib import Path
import cv2
import pickle
from torchvision.transforms import ToTensor
from random import randint
from pathlib import Path
from multiprocessing import Pool


class BCDataset(Dataset):
    def __init__(
        self,
        hist_size,
        pred_horizon,
        data_root: Path,
        mirror_trajs,
        max_goal_time_sample,
    ) -> None:
        super().__init__()
        self.hist_size = hist_size
        self.pred_horizon = pred_horizon
        # assert pred_horizon <= 5
        self.max_goal_time_sample: int = max_goal_time_sample
        self.min_goal_time_sample: int = self.pred_horizon + 1  # 1s at 5hz
        self.get_images = True
        self.trajs_path = list(data_root.glob("*.pkl"))
        # self.trajs_path = self.trajs_path[0:1]
        self.trajs = []
        self.trajs_filename = []
        filenames_to_cut = [
            "soccerfield",
            "donuts_trim_the_end",
            "deep_forest_3_trim_the_end",
        ]
        for p in self.trajs_path:
            filename = p.stem
            self.trajs_filename.append(filename)
            print(f"Reading {filename}...")
            traj = pickle.load(open(p, "rb"))
            self.trajs.append({})
            for k, v in traj.items():
                if "image" in k:
                    dtype = torch.uint8
                else:
                    dtype = torch.float32
                buffer = torch.empty((len(v), *v[0].shape), dtype=dtype)
                for t, vt in enumerate(v):
                    buffer[t] = torch.tensor(vt)
                if str(filename) in filenames_to_cut:
                    print("Cutting traj...")
                    len_buffer = buffer.shape[0]
                    buffer = buffer[: len_buffer // 2]
                self.trajs[-1][k] = buffer
            del traj
        # Remove beginning of trajectories due to heading convergence
        self.trajs = [
            {key: item[25:] for key, item in traj.items()} for traj in self.trajs
        ]
        for i, p in enumerate(data_root.glob("*.pkl")):
            p = p.stem
            if "test" in p:
                # Dataset generated by Faraz is in RGB rather than BGR because of an unknown bug
                for key in ["images_l", "images_r", "images_f"]:
                    self.trajs[i][key] = self.trajs[i][key][:, :, :, [2, 1, 0]]
        self.trajs = [
            {key: item[:-50] for key, item in traj.items()} for traj in self.trajs
        ]
        if mirror_trajs:
            self.trajs += [BCDataset.mirror_traj(traj) for traj in self.trajs]
            self.trajs_filename = self.trajs_filename + self.trajs_filename
        for traj in self.trajs:
            self.generate_delta_pos(traj)
        self.vel_limit_x = 1.5
        self.vel_limit_y = 0.1
        self.vel_limit_z = 0.1
        total_length = sum(
            [BCDataset.get_traj_len_m(traj) for traj in self.trajs[:-12]]
        )
        total_length_s = sum(
            [BCDataset.get_traj_len_s(traj) for traj in self.trajs[:-12]]
        )
        print(f"Total dataset length is {total_length} m over {total_length_s} s.")
        total_length = sum(
            [BCDataset.get_traj_len_m(traj) for traj in self.trajs[-12:]]
        )
        total_length_s = sum(
            [BCDataset.get_traj_len_s(traj) for traj in self.trajs[-12:]]
        )
        print(f"Total ss dataset length is {total_length} m.")
        print(f"Total ss dataset length is {total_length_s} s.")

    @staticmethod
    def generate_delta_pos(traj):
        previous_pos = None
        deltas = [torch.cat([torch.eye(3), torch.zeros((3, 1))], dim=1)]
        for i in range(traj["positions"].shape[0]):
            pos = traj["orientations"][i].clone()
            pos[:3, 3] = traj["positions"][i].clone()
            if previous_pos is not None:
                delta = torch.linalg.inv(previous_pos) @ pos
                deltas.append(delta[:3])
            previous_pos = pos
        traj["velocities"] = torch.stack(deltas)

    @staticmethod
    def get_traj_len_m(traj):
        length = 0.0
        for i in range(traj["velocities"].shape[0]):
            lin = traj["velocities"][i, :3, 3]
            length += np.linalg.norm(lin)
        return length

    @staticmethod
    def get_traj_len_s(traj):
        n_steps = traj["velocities"].shape[0]
        return float(n_steps) / 5.0

    @staticmethod
    def mirror_pos(pos, ori):
        mirror_matrix = torch.tensor(
            [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],
            dtype=torch.float32,
        )
        pose = torch.eye(4)
        pose[:3, :3] = ori[:3, :3]
        pose[:3, 3] = pos
        mirrored_pose = pose @ mirror_matrix
        return mirrored_pose[:3, 3], mirrored_pose

    @staticmethod
    def mirror_traj(traj):
        new_traj = {}
        new_traj["images_l"] = traj["images_r"]
        new_traj["images_r"] = traj["images_l"]
        new_traj["images_f"] = torch.flip(traj["images_f"], dims=(2,))
        new_traj["positions"] = torch.empty_like(traj["positions"])
        new_traj["orientations"] = torch.empty_like(traj["orientations"])
        for i in range(traj["positions"].shape[0]):
            pos = traj["positions"][i].clone()
            ori = traj["orientations"][i].clone()
            pos, ori = BCDataset.mirror_pos(pos, ori)
            new_traj["positions"][i] = pos
            new_traj["orientations"][i] = ori
        new_traj["commands"] = traj["commands"].clone()
        new_traj["commands"][:, 1] *= -1
        return new_traj

    def __len__(self):
        return sum([self.len_traj(traj) for traj in self.trajs])

    def len_traj(self, traj) -> int:
        return traj["positions"].shape[0] - self.hist_size - self.pred_horizon + 1

    def process_img_list(self, imgs):
        imgs = imgs.to(torch.float32) / 255.0
        imgs = imgs.permute(0, 3, 1, 2)
        imgs = imgs[:, [2, 1, 0], :, :]
        return imgs

    def get_mean_std_commands(self):
        data = torch.cat([traj["commands"] for traj in self.trajs], dim=0)
        mu = torch.mean(data, dim=0)
        std = torch.std(data, dim=0)
        # mu = torch.zeros_like(mu)
        # std = torch.ones_like(std)
        return mu, std

    def get_traj_id_and_local_index(self, index):
        tracker = 0
        for i, t_length in enumerate([self.len_traj(traj) for traj in self.trajs]):
            if tracker <= index < tracker + t_length:
                return i, index - tracker
            tracker += t_length
        raise ValueError("")

    def reject(self, vels, command):
        command = torch.sum(command, dim=-1)
        if torch.any(abs(command) < 1e-5):
            return True

        vel_x = vels[:, 0, 3]
        vel_y = vels[:, 1, 3]
        vel_z = vels[:, 2, 3]

        if torch.any(torch.abs(vel_z) > self.vel_limit_z):
            return True

        if torch.any(torch.abs(vel_y) > self.vel_limit_y):
            return True

        if torch.any(torch.abs(vel_x) > self.vel_limit_x):
            return True

    def __getitem__(self, index) -> Any:
        # JF: I hate this. Super hacky. It works. Queried index is ignored (doesn't matter?)
        while True:
            index = randint(0, len(self) - 1)
            traj_id, local_index = self.get_traj_id_and_local_index(index)
            traj = self.trajs[traj_id]
            # Can't use the first few time steps because need history
            index_traj = local_index + self.hist_size - 1
            i_low = index_traj - self.hist_size + 1
            i_high = index_traj + 1  # +1 because of non-inclusive slice

            vels = traj["velocities"][i_low:i_high].clone()
            command = traj["commands"][i_high : i_high + self.pred_horizon].clone()
            past_commands = traj["commands"][i_low:i_high].clone()

            if self.reject(vels, command):
                continue

            if self.get_images:
                imgs_l = traj["images_l"][i_low:i_high].clone()
                imgs_f = traj["images_f"][i_low:i_high].clone()
                imgs_r = traj["images_r"][i_low:i_high].clone()
                imgs_l = self.process_img_list(imgs_l)
                imgs_f = self.process_img_list(imgs_f)
                imgs_r = self.process_img_list(imgs_r)
            else:
                imgs_l, imgs_f, imgs_r = [], [], []

            if "embs" in traj.keys():
                embs = traj["embs"][i_low:i_high].clone()

            position_start = traj["positions"][index_traj].clone()
            orientation_start = traj["orientations"][index_traj].clone()

            goal_sample_time = randint(
                self.min_goal_time_sample, self.max_goal_time_sample
            )
            goal_sample_index = min(
                i_high + goal_sample_time, traj["commands"].shape[0] - 1
            )

            position_goal = traj["positions"][goal_sample_index].clone()

            orientation_start[:3, 3] = position_start

            goal_position = torch.linalg.inv(orientation_start) @ torch.cat(
                [position_goal, torch.ones((1,))]
            )
            goal_position = goal_position[:3]
            info = {
                "traj_id": traj_id,
                "traj_filename": self.trajs_filename[traj_id],
                "local_index": local_index,
            }
            if "embs" in traj.keys():
                info["embs"] = embs

            return (
                imgs_l,
                imgs_f,
                imgs_r,
                vels,
                past_commands,
                goal_position,
                command,
                info,
            )
